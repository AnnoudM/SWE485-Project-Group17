{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Install Required Libraries and Download the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (2.2.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: models\\mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# This will download and return the path to the file\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    local_dir=\"models\"\n",
    ")\n",
    "\n",
    "print(\"Model downloaded to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Load and Clean the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values After Removal:\n",
      " اسم_المدرسة                        0\n",
      "المنطقة_الإدارية                   0\n",
      "الإدارة_التعليمية                  0\n",
      "المكتب_التعليمي                    0\n",
      "السلطة                             0\n",
      "نوع_التعليم                        0\n",
      "الجنس                              0\n",
      "نوع_الاختبار                       0\n",
      "تخصص_الاختبار                      0\n",
      "متوسط_أداء_الطلبة_في_المدرسة       0\n",
      "ترتيب_المدرسة_على_مستوى_المدارس    0\n",
      "dtype: int64\n",
      "Number of rows before cleaning: 6720\n",
      "Number of rows after cleaning: 6706\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>اسم_المدرسة</th>\n",
       "      <th>المنطقة_الإدارية</th>\n",
       "      <th>الإدارة_التعليمية</th>\n",
       "      <th>المكتب_التعليمي</th>\n",
       "      <th>السلطة</th>\n",
       "      <th>نوع_التعليم</th>\n",
       "      <th>الجنس</th>\n",
       "      <th>تخصص_الاختبار</th>\n",
       "      <th>متوسط_أداء_الطلبة_في_المدرسة</th>\n",
       "      <th>ترتيب_المدرسة_على_مستوى_المدارس</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>أم سلمة الثانوية للموهوبات - مقررات</td>\n",
       "      <td>مكة المكرمة</td>\n",
       "      <td>الإدارة العامة للتعليم بمنطقة مكة المكرمة</td>\n",
       "      <td>مكتب التعليم جنوب مكة المكرمة</td>\n",
       "      <td>حكومي</td>\n",
       "      <td>تعليم عام بنات</td>\n",
       "      <td>بنات</td>\n",
       "      <td>علمي</td>\n",
       "      <td>89.622377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ثانوية الرواد الأهلية ( الازدهار ) - مقررات</td>\n",
       "      <td>الرياض</td>\n",
       "      <td>الإدارة العامة للتعليم بمنطقة الرياض</td>\n",
       "      <td>شمال</td>\n",
       "      <td>أهلي</td>\n",
       "      <td>تعليم عام بنات</td>\n",
       "      <td>بنات</td>\n",
       "      <td>نظري</td>\n",
       "      <td>84.440000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ثانوية الموهوبات ( نظام مقررات )</td>\n",
       "      <td>مكة المكرمة</td>\n",
       "      <td>الإدارة العامة للتعليم بمحافظة جدة</td>\n",
       "      <td>مكتب التعليم بشمال جدة</td>\n",
       "      <td>حكومي</td>\n",
       "      <td>تعليم عام بنات</td>\n",
       "      <td>بنات</td>\n",
       "      <td>علمي</td>\n",
       "      <td>89.280423</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>الثانوية السادسة لتحفيظ القرآن الكريم 6 - مقررات</td>\n",
       "      <td>الرياض</td>\n",
       "      <td>الإدارة العامة للتعليم بمنطقة الرياض</td>\n",
       "      <td>شمال</td>\n",
       "      <td>حكومي</td>\n",
       "      <td>تحفيظ بنات</td>\n",
       "      <td>بنات</td>\n",
       "      <td>نظري</td>\n",
       "      <td>83.550000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ثانوية مدارس الظهران الاهلية (مقررات)</td>\n",
       "      <td>الشرقية</td>\n",
       "      <td>الإدارة العامة للتعليم بالمنطقة الشرقية</td>\n",
       "      <td>مكتب التعليم بمحافظة الخبر</td>\n",
       "      <td>أهلي</td>\n",
       "      <td>تعليم عام بنات</td>\n",
       "      <td>بنات</td>\n",
       "      <td>علمي</td>\n",
       "      <td>88.903225</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        اسم_المدرسة المنطقة_الإدارية  \\\n",
       "0               أم سلمة الثانوية للموهوبات - مقررات      مكة المكرمة   \n",
       "1       ثانوية الرواد الأهلية ( الازدهار ) - مقررات           الرياض   \n",
       "2                  ثانوية الموهوبات ( نظام مقررات )      مكة المكرمة   \n",
       "3  الثانوية السادسة لتحفيظ القرآن الكريم 6 - مقررات           الرياض   \n",
       "4             ثانوية مدارس الظهران الاهلية (مقررات)          الشرقية   \n",
       "\n",
       "                           الإدارة_التعليمية                المكتب_التعليمي  \\\n",
       "0  الإدارة العامة للتعليم بمنطقة مكة المكرمة  مكتب التعليم جنوب مكة المكرمة   \n",
       "1       الإدارة العامة للتعليم بمنطقة الرياض                           شمال   \n",
       "2         الإدارة العامة للتعليم بمحافظة جدة         مكتب التعليم بشمال جدة   \n",
       "3       الإدارة العامة للتعليم بمنطقة الرياض                           شمال   \n",
       "4    الإدارة العامة للتعليم بالمنطقة الشرقية     مكتب التعليم بمحافظة الخبر   \n",
       "\n",
       "  السلطة     نوع_التعليم الجنس تخصص_الاختبار  متوسط_أداء_الطلبة_في_المدرسة  \\\n",
       "0  حكومي  تعليم عام بنات  بنات          علمي                     89.622377   \n",
       "1   أهلي  تعليم عام بنات  بنات          نظري                     84.440000   \n",
       "2  حكومي  تعليم عام بنات  بنات          علمي                     89.280423   \n",
       "3  حكومي      تحفيظ بنات  بنات          نظري                     83.550000   \n",
       "4   أهلي  تعليم عام بنات  بنات          علمي                     88.903225   \n",
       "\n",
       "   ترتيب_المدرسة_على_مستوى_المدارس  \n",
       "0                                1  \n",
       "1                                1  \n",
       "2                                2  \n",
       "3                                2  \n",
       "4                                3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../Dataset/Averages in in General Aptitude Tests aa.csv\" )\n",
    "\n",
    "# Now we will remove all rows that have any missing values\n",
    "df_cleaned = df.dropna()  # dropna() removes any row that contains a missing value\n",
    "\n",
    "# After removing, We need to check if there are still any missing values\n",
    "print(\"Missing Values After Removal:\\n\", df_cleaned.isnull().sum())\n",
    "\n",
    "# Also, We will check the number of rows left after removing missing values\n",
    "print(f\"Number of rows before cleaning: {df.shape[0]}\")\n",
    "print(f\"Number of rows after cleaning: {df_cleaned.shape[0]}\")\n",
    "# Remove the \"نوع_الاختبار\" column as it contains only one unique value\n",
    "df_cleaned = df_cleaned.drop(columns=[\"نوع_الاختبار\"])\n",
    "\n",
    "# Check the first few rows to confirm the column is removed\n",
    "df_cleaned.head()\n",
    "# Display the top 5 rows\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Load the LLaMA Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_cpp import Llama  # Import the model class\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",  # Path to the model file\n",
    "    n_ctx=2048,    # Context length\n",
    "    n_threads=4,   # Number of CPU threads\n",
    "    verbose=True   # Print detailed logs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Define Model Response Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send prompt to the model and get a response\n",
    "def ask_model(prompt):\n",
    "    response = llm(\n",
    "        prompt=f\"[INST] {prompt} [/INST]\",  # Format prompt\n",
    "        max_tokens=1024,  # Max length of output\n",
    "        temperature=0.7,  # Randomness level\n",
    "        stop=[\"</s>\"]     # Stop token\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"].strip()  # Return clean text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5: Define Filtering Logic for User Preferences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique values for filterable columns\n",
    "\n",
    "regions = df_cleaned[\"المنطقة_الإدارية\"].unique().tolist()\n",
    "authorities = df_cleaned[\"السلطة\"].unique().tolist()\n",
    "genders = df_cleaned[\"الجنس\"].unique().tolist()\n",
    "edu_types = df_cleaned[\"نوع_التعليم\"].unique().tolist()\n",
    "districts = df_cleaned[\"الإدارة_التعليمية\"].unique().tolist()\n",
    "offices = df_cleaned[\"المكتب_التعليمي\"].unique().tolist()\n",
    "\n",
    "# Extract filters based on user's input\n",
    "def extract_filters(user_input):\n",
    "    filters = {}\n",
    "    \n",
    " # Check if any option exists in the input text\n",
    "    def partial_match(text, options):\n",
    "        for option in options:\n",
    "            if option in text:\n",
    "                return option\n",
    "        return None\n",
    "    \n",
    "    # Map each column to its values\n",
    "    filters_map = {\n",
    "        \"المنطقة_الإدارية\": regions,\n",
    "        \"السلطة\": authorities,\n",
    "        \"الجنس\": genders,\n",
    "        \"نوع_التعليم\": edu_types,\n",
    "        \"الإدارة_التعليمية\": districts,\n",
    "        \"المكتب_التعليمي\": offices\n",
    "    }\n",
    "    \n",
    "    # Loop through the map and find matches in the input\n",
    "    for column, values in filters_map.items():\n",
    "        match = partial_match(user_input, values)\n",
    "        if match:\n",
    "            filters[column] = match\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 6: Apply Filters and Select Top Schools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الفلاتر المستخرجة: {'المنطقة_الإدارية': 'الرياض', 'السلطة': 'حكومي', 'الجنس': 'بنات', 'المكتب_التعليمي': 'شمال'}\n"
     ]
    }
   ],
   "source": [
    "# Sample input from the user\n",
    "user_input = \"طالبة في الرياض، تعليم عام، حكومية، بنات، مكتب التعليم شمال الرياض\"\n",
    "\n",
    "\n",
    "# Extract relevant filters from the input\n",
    "filters = extract_filters(user_input)\n",
    "print(\"الفلاتر المستخرجة:\", filters)\n",
    "\n",
    "# Filter the dataset using the extracted filters\n",
    "filtered_df = df_cleaned.copy()\n",
    "for col, value in filters.items():\n",
    "    filtered_df = filtered_df[filtered_df[col] == value]\n",
    "    \n",
    "# Get top 3 schools based on best ranking (lowest number = best)\n",
    "top_schools = filtered_df.sort_values(by=\"ترتيب_المدرسة_على_مستوى_المدارس\", ascending=True).head(3)\n",
    "\n",
    "\n",
    "# Prepare selected school info to be passed into the prompt\n",
    "school_info = top_schools[\n",
    "    [\"اسم_المدرسة\", \"المنطقة_الإدارية\", \"نوع_التعليم\", \"ترتيب_المدرسة_على_مستوى_المدارس\", \"متوسط_أداء_الطلبة_في_المدرسة\"]\n",
    "].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 7: Create the Prompts to Send to the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompt that will be sent to the language model\n",
    "def template_1(user_input, schools):\n",
    "    # Start with the student description\n",
    "    prompt = f\"وصف الطالبة:\\n{user_input}\\n\\n\"\n",
    "\n",
    "    # Add the list of top 3 recommended schools (names only)\n",
    "    prompt += \"قائمة المدارس المرشحة:\\n\\n\"\n",
    "    for i, school in enumerate(schools[:3], 1):\n",
    "        prompt += f\"{i}. {school['اسم_المدرسة']}\\n\"\n",
    "\n",
    "    # Final instruction to guide the response\n",
    "    prompt += (\n",
    "        \"\\nمن بين هذه المدارس، ما الأنسب للطالبة؟\\n\"\n",
    "        \" اكتب:\\n\"\n",
    "        \"- اسم المدرسة فقط.\\n\"\n",
    "        \"- السبب باختصار في سطر.\\n\"\n",
    "        \" الرد يكون باللغة العربية الفصحى فقط، كل معلومة في سطر مختلف، بدون تكرار أو إعادة ذكر القائمة.\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompt that will be sent to the language model\n",
    "def template_2(user_input, schools):\n",
    "    prompt = f\" وصف الطالبة:\\n{user_input}\\n\\n\"\n",
    "    prompt += \" قائمة المدارس المرشحة:\\n\"\n",
    "\n",
    "    # Loop through the top 3 schools\n",
    "    for i, school in enumerate(schools[:3], 1):\n",
    "        prompt += (\n",
    "            f\"{i}. اسم المدرسة: {school['اسم_المدرسة']}\\n\"\n",
    "            f\"   المنطقة: {school['المنطقة_الإدارية']}\\n\"\n",
    "            f\"   نوع التعليم: {school['نوع_التعليم']}\\n\"\n",
    "            f\"   ترتيب المدرسة: {school['ترتيب_المدرسة_على_مستوى_المدارس']}\\n\\n\"\n",
    "            f\"   متوسط الأداء الأكاديمي: {school['متوسط_أداء_الطلبة_في_المدرسة']:.2f}\\n\\n\"\n",
    "\n",
    "        )\n",
    "\n",
    "    # Final instruction to the language model\n",
    "    prompt += (\n",
    "        \"اختر المدرسة صاحبة الترتيب الأكاديمي الأفضل (أي أصغر رقم).\\n\"\n",
    "        \"اكتب:\\n\"\n",
    "        \"- اسم المدرسة\\n\"\n",
    "        \"- الترتيب\\n\"\n",
    "        \"- متوسط الأداء الأكاديمي\\n\"\n",
    "        \"- سبب الاختيار بناءً على وصف الطالبة\\n\"\n",
    "        \" الرد يكون بالعربية الفصحى فقط، بدون تكرار أو إعادة القائمة.\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 8: Send Prompts to the Model and Display Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   35705.52 ms\n",
      "llama_perf_context_print: prompt eval time =   35704.54 ms /   372 tokens (   95.98 ms per token,    10.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20064.21 ms /   107 runs   (  187.52 ms per token,     5.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   55841.33 ms /   479 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMPLATE 1 RESPONSE ===\n",
      " - الثانوية السادسة لتحفيظ القرءان الكريم 6 - مقررات\n",
      "- ثانوية جامعة الأميرة نورة - مقررات\n",
      "- الثانوية الخامسة والعشرون 25 - مقررات\n"
     ]
    }
   ],
   "source": [
    "# Generate the prompt using the student's input and top schools\n",
    "prompt1 = template_1(user_input, school_info)\n",
    "\n",
    "# Send the prompt to the LLaMA model to get a response\n",
    "response_1 = ask_model(prompt1)\n",
    "\n",
    "# Display the model's response\n",
    "print(\"=== TEMPLATE 1 RESPONSE ===\\n\", response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 677 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   74323.92 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   92526.73 ms /   352 runs   (  262.86 ms per token,     3.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   93010.22 ms /   353 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMPLATE 2 RESPONSE ===\n",
      " رد:\n",
      "- اسم المدرسة: الثانوية السادسة لتحفيظ القرءان الكريم 6 - مقررات\n",
      "- الترتيب: 2\n",
      "- متوسط الأداء الأكاديمي: 83.55\n",
      "\n",
      "رد:\n",
      "- اسم المدرسة: ثانوية جامعة الأميرة نورة - مقررات\n",
      "- الترتيب: 14\n",
      "- متوسط الأداء الأكاديمي: 83.34\n",
      "\n",
      "رد:\n",
      "- اسم المدرسة: الثانوية الخامسة والعشرون 25- مقررات\n",
      "- الترتيب: 41\n",
      "- متوسط الأداء الأكاديمي: 81.15\n",
      "\n",
      "رد:\n",
      "- بناءً على وصف الطالبة في الرياض، تعليم عام، حكومية، بنات، مكتب التعليم شمال الرياض.\n"
     ]
    }
   ],
   "source": [
    "# Generate the prompt using the student's input and top schools\n",
    "prompt2 = template_2(user_input, school_info)\n",
    "\n",
    "# Send the prompt to the LLaMA model to get a response\n",
    "response_2 = ask_model(prompt2)\n",
    "\n",
    "# Display the model's response\n",
    "print(\"=== TEMPLATE 2 RESPONSE ===\\n\", response_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
