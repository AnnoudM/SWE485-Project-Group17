{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (2.2.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: models\\mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# This will download and return the path to the file\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    local_dir=\"models\"\n",
    ")\n",
    "\n",
    "print(\"Model downloaded to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values After Removal:\n",
      " Ø§Ø³Ù…_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©                        0\n",
      "Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©                   0\n",
      "Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©                  0\n",
      "Ø§Ù„Ù…ÙƒØªØ¨_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ                    0\n",
      "Ø§Ù„Ø³Ù„Ø·Ø©                             0\n",
      "Ù†ÙˆØ¹_Ø§Ù„ØªØ¹Ù„ÙŠÙ…                        0\n",
      "Ø§Ù„Ø¬Ù†Ø³                              0\n",
      "Ù†ÙˆØ¹_Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±                       0\n",
      "ØªØ®ØµØµ_Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±                      0\n",
      "Ù…ØªÙˆØ³Ø·_Ø£Ø¯Ø§Ø¡_Ø§Ù„Ø·Ù„Ø¨Ø©_ÙÙŠ_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©       0\n",
      "ØªØ±ØªÙŠØ¨_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©_Ø¹Ù„Ù‰_Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ù…Ø¯Ø§Ø±Ø³    0\n",
      "dtype: int64\n",
      "Number of rows before cleaning: 6720\n",
      "Number of rows after cleaning: 6706\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ø§Ø³Ù…_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©</th>\n",
       "      <th>Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©</th>\n",
       "      <th>Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©</th>\n",
       "      <th>Ø§Ù„Ù…ÙƒØªØ¨_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ</th>\n",
       "      <th>Ø§Ù„Ø³Ù„Ø·Ø©</th>\n",
       "      <th>Ù†ÙˆØ¹_Ø§Ù„ØªØ¹Ù„ÙŠÙ…</th>\n",
       "      <th>Ø§Ù„Ø¬Ù†Ø³</th>\n",
       "      <th>ØªØ®ØµØµ_Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±</th>\n",
       "      <th>Ù…ØªÙˆØ³Ø·_Ø£Ø¯Ø§Ø¡_Ø§Ù„Ø·Ù„Ø¨Ø©_ÙÙŠ_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©</th>\n",
       "      <th>ØªØ±ØªÙŠØ¨_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©_Ø¹Ù„Ù‰_Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ù…Ø¯Ø§Ø±Ø³</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø£Ù… Ø³Ù„Ù…Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ù…ÙˆÙ‡ÙˆØ¨Ø§Øª - Ù…Ù‚Ø±Ø±Ø§Øª</td>\n",
       "      <td>Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©</td>\n",
       "      <td>Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©</td>\n",
       "      <td>Ù…ÙƒØªØ¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¬Ù†ÙˆØ¨ Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©</td>\n",
       "      <td>Ø­ÙƒÙˆÙ…ÙŠ</td>\n",
       "      <td>ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¹Ù„Ù…ÙŠ</td>\n",
       "      <td>89.622377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ø±ÙˆØ§Ø¯ Ø§Ù„Ø£Ù‡Ù„ÙŠØ© ( Ø§Ù„Ø§Ø²Ø¯Ù‡Ø§Ø± ) - Ù…Ù‚Ø±Ø±Ø§Øª</td>\n",
       "      <td>Ø§Ù„Ø±ÙŠØ§Ø¶</td>\n",
       "      <td>Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶</td>\n",
       "      <td>Ø´Ù…Ø§Ù„</td>\n",
       "      <td>Ø£Ù‡Ù„ÙŠ</td>\n",
       "      <td>ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ù†Ø¸Ø±ÙŠ</td>\n",
       "      <td>84.440000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ù…ÙˆÙ‡ÙˆØ¨Ø§Øª ( Ù†Ø¸Ø§Ù… Ù…Ù‚Ø±Ø±Ø§Øª )</td>\n",
       "      <td>Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©</td>\n",
       "      <td>Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ø­Ø§ÙØ¸Ø© Ø¬Ø¯Ø©</td>\n",
       "      <td>Ù…ÙƒØªØ¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø´Ù…Ø§Ù„ Ø¬Ø¯Ø©</td>\n",
       "      <td>Ø­ÙƒÙˆÙ…ÙŠ</td>\n",
       "      <td>ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¹Ù„Ù…ÙŠ</td>\n",
       "      <td>89.280423</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ø³Ø§Ø¯Ø³Ø© Ù„ØªØ­ÙÙŠØ¸ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… 6 - Ù…Ù‚Ø±Ø±Ø§Øª</td>\n",
       "      <td>Ø§Ù„Ø±ÙŠØ§Ø¶</td>\n",
       "      <td>Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶</td>\n",
       "      <td>Ø´Ù…Ø§Ù„</td>\n",
       "      <td>Ø­ÙƒÙˆÙ…ÙŠ</td>\n",
       "      <td>ØªØ­ÙÙŠØ¸ Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ù†Ø¸Ø±ÙŠ</td>\n",
       "      <td>83.550000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø«Ø§Ù†ÙˆÙŠØ© Ù…Ø¯Ø§Ø±Ø³ Ø§Ù„Ø¸Ù‡Ø±Ø§Ù† Ø§Ù„Ø§Ù‡Ù„ÙŠØ© (Ù…Ù‚Ø±Ø±Ø§Øª)</td>\n",
       "      <td>Ø§Ù„Ø´Ø±Ù‚ÙŠØ©</td>\n",
       "      <td>Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©</td>\n",
       "      <td>Ù…ÙƒØªØ¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ø­Ø§ÙØ¸Ø© Ø§Ù„Ø®Ø¨Ø±</td>\n",
       "      <td>Ø£Ù‡Ù„ÙŠ</td>\n",
       "      <td>ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¨Ù†Ø§Øª</td>\n",
       "      <td>Ø¹Ù„Ù…ÙŠ</td>\n",
       "      <td>88.903225</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Ø§Ø³Ù…_Ø§Ù„Ù…Ø¯Ø±Ø³Ø© Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©  \\\n",
       "0               Ø£Ù… Ø³Ù„Ù…Ø© Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ù„Ù„Ù…ÙˆÙ‡ÙˆØ¨Ø§Øª - Ù…Ù‚Ø±Ø±Ø§Øª      Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©   \n",
       "1       Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ø±ÙˆØ§Ø¯ Ø§Ù„Ø£Ù‡Ù„ÙŠØ© ( Ø§Ù„Ø§Ø²Ø¯Ù‡Ø§Ø± ) - Ù…Ù‚Ø±Ø±Ø§Øª           Ø§Ù„Ø±ÙŠØ§Ø¶   \n",
       "2                  Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ù…ÙˆÙ‡ÙˆØ¨Ø§Øª ( Ù†Ø¸Ø§Ù… Ù…Ù‚Ø±Ø±Ø§Øª )      Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©   \n",
       "3  Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ø³Ø§Ø¯Ø³Ø© Ù„ØªØ­ÙÙŠØ¸ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… 6 - Ù…Ù‚Ø±Ø±Ø§Øª           Ø§Ù„Ø±ÙŠØ§Ø¶   \n",
       "4             Ø«Ø§Ù†ÙˆÙŠØ© Ù…Ø¯Ø§Ø±Ø³ Ø§Ù„Ø¸Ù‡Ø±Ø§Ù† Ø§Ù„Ø§Ù‡Ù„ÙŠØ© (Ù…Ù‚Ø±Ø±Ø§Øª)          Ø§Ù„Ø´Ø±Ù‚ÙŠØ©   \n",
       "\n",
       "                           Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©                Ø§Ù„Ù…ÙƒØªØ¨_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ  \\\n",
       "0  Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©  Ù…ÙƒØªØ¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¬Ù†ÙˆØ¨ Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©   \n",
       "1       Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶                           Ø´Ù…Ø§Ù„   \n",
       "2         Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ø­Ø§ÙØ¸Ø© Ø¬Ø¯Ø©         Ù…ÙƒØªØ¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø´Ù…Ø§Ù„ Ø¬Ø¯Ø©   \n",
       "3       Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶                           Ø´Ù…Ø§Ù„   \n",
       "4    Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø´Ø±Ù‚ÙŠØ©     Ù…ÙƒØªØ¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ø­Ø§ÙØ¸Ø© Ø§Ù„Ø®Ø¨Ø±   \n",
       "\n",
       "  Ø§Ù„Ø³Ù„Ø·Ø©     Ù†ÙˆØ¹_Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¬Ù†Ø³ ØªØ®ØµØµ_Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±  Ù…ØªÙˆØ³Ø·_Ø£Ø¯Ø§Ø¡_Ø§Ù„Ø·Ù„Ø¨Ø©_ÙÙŠ_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©  \\\n",
       "0  Ø­ÙƒÙˆÙ…ÙŠ  ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª  Ø¨Ù†Ø§Øª          Ø¹Ù„Ù…ÙŠ                     89.622377   \n",
       "1   Ø£Ù‡Ù„ÙŠ  ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª  Ø¨Ù†Ø§Øª          Ù†Ø¸Ø±ÙŠ                     84.440000   \n",
       "2  Ø­ÙƒÙˆÙ…ÙŠ  ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª  Ø¨Ù†Ø§Øª          Ø¹Ù„Ù…ÙŠ                     89.280423   \n",
       "3  Ø­ÙƒÙˆÙ…ÙŠ      ØªØ­ÙÙŠØ¸ Ø¨Ù†Ø§Øª  Ø¨Ù†Ø§Øª          Ù†Ø¸Ø±ÙŠ                     83.550000   \n",
       "4   Ø£Ù‡Ù„ÙŠ  ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù… Ø¨Ù†Ø§Øª  Ø¨Ù†Ø§Øª          Ø¹Ù„Ù…ÙŠ                     88.903225   \n",
       "\n",
       "   ØªØ±ØªÙŠØ¨_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©_Ø¹Ù„Ù‰_Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ù…Ø¯Ø§Ø±Ø³  \n",
       "0                                1  \n",
       "1                                1  \n",
       "2                                2  \n",
       "3                                2  \n",
       "4                                3  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phase 4: Generative AI Integration\n",
    "# Filename: phase4_generative_ai.ipynb\n",
    "\n",
    "# ===============================\n",
    "# ğŸ“¦ STEP 1: Load the dataset\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"../Dataset/Averages in in General Aptitude Tests aa.csv\" )\n",
    "\n",
    "# Now we will remove all rows that have any missing values\n",
    "df_cleaned = df.dropna()  # dropna() removes any row that contains a missing value\n",
    "\n",
    "# After removing, We need to check if there are still any missing values\n",
    "print(\"Missing Values After Removal:\\n\", df_cleaned.isnull().sum())\n",
    "\n",
    "# Also, We will check the number of rows left after removing missing values\n",
    "print(f\"Number of rows before cleaning: {df.shape[0]}\")\n",
    "print(f\"Number of rows after cleaning: {df_cleaned.shape[0]}\")\n",
    "# Remove the \"Ù†ÙˆØ¹_Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\" column as it contains only one unique value\n",
    "df_cleaned = df_cleaned.drop(columns=[\"Ù†ÙˆØ¹_Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\"])\n",
    "\n",
    "# Check the first few rows to confirm the column is removed\n",
    "df_cleaned.head()\n",
    "# Display the top 5 rows\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",  \n",
    "    n_ctx=2048,\n",
    "    n_threads=4, \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_model(prompt):\n",
    "    response = llm(\n",
    "        prompt=f\"[INST] {prompt} [/INST]\",\n",
    "        max_tokens=512,\n",
    "        temperature=0.7,\n",
    "        stop=[\"</s>\"]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regions = df_cleaned[\"Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©\"].unique().tolist()\n",
    "authorities = df_cleaned[\"Ø§Ù„Ø³Ù„Ø·Ø©\"].unique().tolist()\n",
    "genders = df_cleaned[\"Ø§Ù„Ø¬Ù†Ø³\"].unique().tolist()\n",
    "edu_types = df_cleaned[\"Ù†ÙˆØ¹_Ø§Ù„ØªØ¹Ù„ÙŠÙ…\"].unique().tolist()\n",
    "districts = df_cleaned[\"Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©\"].unique().tolist()\n",
    "offices = df_cleaned[\"Ø§Ù„Ù…ÙƒØªØ¨_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ\"].unique().tolist()\n",
    "\n",
    "\n",
    "\n",
    "def extract_filters(user_input):\n",
    "    filters = {}\n",
    "    \n",
    "    for region in regions:\n",
    "        if region in user_input.lower():\n",
    "            filters[\"Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©\"] = region\n",
    "    for authority in authorities:\n",
    "        if authority in user_input:\n",
    "            filters[\"Ø§Ù„Ø³Ù„Ø·Ø©\"] = authority\n",
    "    for gender in genders:\n",
    "        if gender in user_input:\n",
    "            filters[\"Ø§Ù„Ø¬Ù†Ø³\"] = gender\n",
    "    for edu_type in edu_types:\n",
    "        if edu_type in user_input:\n",
    "            filters[\"Ù†ÙˆØ¹_Ø§Ù„ØªØ¹Ù„ÙŠÙ…\"] = edu_type\n",
    "            \n",
    "    for district in districts:\n",
    "        if district in user_input:\n",
    "            filters[\"Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©\"] = district\n",
    "    for office in offices:\n",
    "        if office in user_input:\n",
    "            filters[\"Ø§Ù„Ù…ÙƒØªØ¨_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ\"] = office\n",
    "    \n",
    "    return filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {'Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©': 'Ø§Ù„Ø±ÙŠØ§Ø¶', 'Ø§Ù„Ø³Ù„Ø·Ø©': 'Ø­ÙƒÙˆÙ…ÙŠ', 'Ø§Ù„Ø¬Ù†Ø³': 'Ø¨Ù†Ø§Øª', 'Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©': 'Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶', 'Ø§Ù„Ù…ÙƒØªØ¨_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ': 'Ø§Ù„Ø±ÙŠØ§Ø¶'}\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Ø·Ø§Ù„Ø¨Ø© ØªØ³ÙƒÙ† ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ù…ÙƒØªØ¨ ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø´Ù…Ø§Ù„ØŒ ÙÙŠ Ù…Ø¯Ø±Ø³Ø© Ø­ÙƒÙˆÙ…ÙŠØ© Ù„Ù„Ø¨Ù†Ø§ØªØŒ Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‡Ùˆ ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù…ØŒ ÙˆØªØ¨Ø­Ø« Ø¹Ù† Ù…Ø¯Ø±Ø³Ø© Ø°Ø§Øª Ø£Ø¯Ø§Ø¡ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…Ø±ØªÙØ¹.\"\n",
    "\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù‚ÙŠÙ…\n",
    "filters = extract_filters(user_input)\n",
    "print(\"Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:\", filters)\n",
    "\n",
    "# ÙÙ„ØªØ±Ø© Ø§Ù„Ø¯Ø§ØªØ§\n",
    "filtered_df = df_cleaned.copy()\n",
    "for col, value in filters.items():\n",
    "    filtered_df = filtered_df[filtered_df[col] == value]\n",
    "\n",
    "top_schools = filtered_df.sort_values(by=\"ØªØ±ØªÙŠØ¨_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©_Ø¹Ù„Ù‰_Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ù…Ø¯Ø§Ø±Ø³\", ascending=True).head(5)\n",
    "\n",
    "school_info = top_schools[\n",
    "    [\"Ø§Ø³Ù…_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\", \"Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©\", \"Ù†ÙˆØ¹_Ø§Ù„ØªØ¹Ù„ÙŠÙ…\", \"ØªØ±ØªÙŠØ¨_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©_Ø¹Ù„Ù‰_Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ù…Ø¯Ø§Ø±Ø³\"]\n",
    "].to_dict(orient=\"records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_1(user_input, schools):\n",
    "    prompt = f\"ğŸ“ ÙˆØµÙ Ø§Ù„Ø·Ø§Ù„Ø¨Ø©:\\n{user_input}\\n\\n\"\n",
    "    prompt += \"ğŸ« Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ø§Ù„Ù…Ø±Ø´Ø­Ø©:\\n\"\n",
    "    \n",
    "    for i, school in enumerate(schools, 1):\n",
    "        prompt += (\n",
    "            f\"{i}. Ø§Ø³Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø©: {school['Ø§Ø³Ù…_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©']}\\n\"\n",
    "            f\"   Ø§Ù„Ù…Ù†Ø·Ù‚Ø©: {school['Ø§Ù„Ù…Ù†Ø·Ù‚Ø©_Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©']}\\n\"\n",
    "            f\"   Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…: {school['Ù†ÙˆØ¹_Ø§Ù„ØªØ¹Ù„ÙŠÙ…']}\\n\"\n",
    "            f\"   ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©: {school['ØªØ±ØªÙŠØ¨_Ø§Ù„Ù…Ø¯Ø±Ø³Ø©_Ø¹Ù„Ù‰_Ù…Ø³ØªÙˆÙ‰_Ø§Ù„Ù…Ø¯Ø§Ø±Ø³']}\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    prompt += (\n",
    "        \"ğŸ“Œ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ØµØ§Ø­Ø¨Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø£ÙŠ Ø§Ù„Ø£ØµØºØ± Ø±Ù‚Ù…Ù‹Ø§) ÙˆÙˆØ¶Ø­ Ù„Ù…Ø§Ø°Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù†Ø³Ø¨ Ù„Ù„Ø·Ø§Ù„Ø¨Ø©.\\n\"\n",
    "        \"ğŸ“ Ø§Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ÙˆØ§Ù„Ø³Ø¨Ø¨ØŒ Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 203 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  203267.91 ms\n",
      "llama_perf_context_print: prompt eval time =   16718.62 ms /   133 tokens (  125.70 ms per token,     7.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   98156.99 ms /   328 runs   (  299.26 ms per token,     3.34 tokens per second)\n",
      "llama_perf_context_print:       total time =  115472.67 ms /   461 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMPLATE 1 RESPONSE ===\n",
      " ğŸ“ ÙˆØµÙ Ø§Ù„Ø·Ø§Ù„Ø¨Ø©:\n",
      "Ø·Ø§Ù„Ø¨Ø© ØªØ³ÙƒÙ† ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ ØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ù…ÙƒØªØ¨ ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø´Ù…Ø§Ù„ØŒ ÙÙŠ Ù…Ø¯Ø±Ø³Ø© Ø­ÙƒÙˆÙ…ÙŠØ© Ù„Ù„Ø¨Ù†Ø§ØªØŒ Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ù‡Ùˆ ØªØ¹Ù„ÙŠÙ… Ø¹Ø§Ù…ØŒ ÙˆØªØ¨Ø­Ø« Ø¹Ù† Ù…Ø¯Ø±Ø³Ø© Ø°Ø§Øª Ø£Ø¯Ø§Ø¡ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ù…Ø±ØªÙØ¹.\n",
      "\n",
      "ğŸ« Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¯Ø§Ø±Ø³ Ø§Ù„Ù…Ø±Ø´Ø­Ø©:\n",
      "ğŸ“Œ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ØµØ§Ø­Ø¨Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¹Ù„Ù‰ (Ø£ÙŠ Ø§Ù„Ø£ØµØºØ± Ø±Ù‚Ù…Ù‹Ø§) ÙˆÙˆØ¶Ø­ Ù„Ù…Ø§Ø°Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ù†Ø³Ø¨ Ù„Ù„Ø·Ø§Ù„Ø¨Ø©.\n",
      "ğŸ“ Ø§Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ÙˆØ§Ù„Ø³Ø¨Ø¨ØŒ Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.\n"
     ]
    }
   ],
   "source": [
    "# Ù†Ø¯Ø§Ø¡ LLaMA\n",
    "prompt1 = template_1(user_input, school_info)\n",
    "response_1 = ask_model(prompt1)\n",
    "print(\"=== TEMPLATE 1 RESPONSE ===\\n\", response_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
