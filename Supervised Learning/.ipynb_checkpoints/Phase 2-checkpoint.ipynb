{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9f501b-71c9-49ae-97e8-3921e2ab1589",
   "metadata": {},
   "source": [
    "## **Step 1: Loading the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35ecf77-982d-407a-ae4d-00cd391fce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>اسم_المدرسة</th>\n",
       "      <th>المنطقة_الإدارية</th>\n",
       "      <th>الإدارة_التعليمية</th>\n",
       "      <th>المكتب_التعليمي</th>\n",
       "      <th>السلطة</th>\n",
       "      <th>نوع_التعليم</th>\n",
       "      <th>الجنس</th>\n",
       "      <th>تخصص_الاختبار</th>\n",
       "      <th>متوسط_أداء_الطلبة_في_المدرسة</th>\n",
       "      <th>ترتيب_المدرسة_على_مستوى_المدارس</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>294</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2496</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.872710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3050</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>218</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991601</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1276</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850850</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3938</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   اسم_المدرسة  المنطقة_الإدارية  الإدارة_التعليمية  المكتب_التعليمي  السلطة  \\\n",
       "0           28                11                 45              294       3   \n",
       "1         2496                 3                 38               71       0   \n",
       "2         3050                11                 34              218       3   \n",
       "3         1276                 3                 38               71       3   \n",
       "4         3938                 4                 31              245       0   \n",
       "\n",
       "   نوع_التعليم  الجنس  تخصص_الاختبار  متوسط_أداء_الطلبة_في_المدرسة  \\\n",
       "0            6      0              0                      1.000000   \n",
       "1            6      0              1                      0.872710   \n",
       "2            6      0              0                      0.991601   \n",
       "3            0      0              1                      0.850850   \n",
       "4            6      0              0                      0.982336   \n",
       "\n",
       "   ترتيب_المدرسة_على_مستوى_المدارس  \n",
       "0                                1  \n",
       "1                                1  \n",
       "2                                2  \n",
       "3                                2  \n",
       "4                                3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we're importing pandas, it's the library we need to work with the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Now, we load the dataset from the CSV file using pandas\n",
    "df = pd.read_csv(r\"../Dataset/Cleaned_Averages.csv\")\n",
    "\n",
    "# We display the first few rows of the dataset to understand its structure and confirm that the data loaded correctly\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c283fe4-7c4a-4f8b-bd18-ddfdcd92a1ef",
   "metadata": {},
   "source": [
    "**Loading the Dataset:** In this step, we import the pandas library to handle the dataset. We use **pd.read_csv()** to load our data from a CSV file into a pandas DataFrame. Then, we call **df.head()** to display the first few rows of the dataset to confirm that it has loaded correctly and that the data structure looks fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431a3fb-7b40-4daf-82d1-b6bf58877814",
   "metadata": {},
   "source": [
    "## **Step 2: Splitting the Data into Training and Testing Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ce5a82-7e86-4be3-8112-4033a1095ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (5364, 7), Test set size: (1342, 7)\n"
     ]
    }
   ],
   "source": [
    "# We select the input features (X) \n",
    "X = df[['المنطقة_الإدارية', 'المكتب_التعليمي', 'السلطة', 'الإدارة_التعليمية', 'نوع_التعليم', 'الجنس', 'متوسط_أداء_الطلبة_في_المدرسة']]\n",
    "\n",
    "# The target (y) is what we want to predict\n",
    "y = df['ترتيب_المدرسة_على_مستوى_المدارس']  \n",
    "\n",
    "# Now, we split the data into training and testing sets (80% for training, 20% for testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Checking the size of the sets\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93738ac3-32f3-4419-935c-7f925875cf1f",
   "metadata": {},
   "source": [
    "**Splitting the Data:** After selecting the features and the target variable, we divide our data into training and test sets using **train_test_split()** from sklearn. The training set will consist of 80% of the data, and the remaining 20% will be used for testing the model. This is an important step because we want to train the model on one set of data and evaluate it on another to check how well it performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ea983-8f16-4fed-bbb5-302a6f8e9447",
   "metadata": {},
   "source": [
    "## **Step 3: Random Forest Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ea69c7-dc4b-416b-817e-fe04e5ba6314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Random Forest): 276.6023270118728\n",
      "MAE (Random Forest): 174.3432041728763\n",
      "R² (Random Forest): 0.8053589667131565\n"
     ]
    }
   ],
   "source": [
    "# First, we import RandomForestRegressor and some metrics we need for evaluation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Now, we create the Random Forest model\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the model using the training data\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the test data\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Calculating RMSE (Root Mean Squared Error) to evaluate the model\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf)**0.5\n",
    "# Calculating MAE (Mean Absolute Error) to evaluate the model\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "# Calculating R² (R-squared) to evaluate the model\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"RMSE (Random Forest): {rmse_rf}\")\n",
    "print(f\"MAE (Random Forest): {mae_rf}\")\n",
    "print(f\"R² (Random Forest): {r2_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d9050-ad90-498f-b42c-7619dcc4fc58",
   "metadata": {},
   "source": [
    "**Random Forest Algorithm:** In this step, we use the Random Forest algorithm to build our model. We set **n_estimators=100** to use 100 trees in the forest and **random_state=42** for reproducibility. After training the model, we use it to predict the school rankings on the test set.\n",
    "\n",
    "**We calculate several evaluation metrics:**\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** This gives us an idea of how far off our predictions are on average. A lower RMSE indicates better performance.\n",
    "\n",
    "- **MAE (Mean Absolute Error):** This shows the average error in our predictions.\n",
    "\n",
    "- **R² (R-squared):** This tells us how well our model is explaining the variance in the target variable. A higher R² means the model fits the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc04b37-90ba-41d2-895e-2d9716cdf190",
   "metadata": {},
   "source": [
    "## **Step 4: XGBoost Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41c5056-5981-4a67-88c9-bf7330050019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (XGBoost): 288.62883009585164\n",
      "MAE (XGBoost): 180.7254638671875\n",
      "R² (XGBoost): 0.7880652546882629\n"
     ]
    }
   ],
   "source": [
    "# We import the XGBoost model and the metrics for evaluation\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Creating the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Training the model using the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions using the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE (Root Mean Squared Error) for XGBoost\n",
    "rmse_xgb = mean_squared_error(y_test, y_pred_xgb)**0.5\n",
    "# Calculating MAE (Mean Absolute Error) for XGBoost\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "# Calculating R² (R-squared) for XGBoost\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"RMSE (XGBoost): {rmse_xgb}\")\n",
    "print(f\"MAE (XGBoost): {mae_xgb}\")\n",
    "print(f\"R² (XGBoost): {r2_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e39b7-f6b2-47dd-ace1-2a4a0bec961c",
   "metadata": {},
   "source": [
    "- **XGBoost Algorithm:** In this step, we are using the XGBoost model, which is known for being highly effective in many regression tasks. We train the model using the training data and make predictions using the test data.\n",
    "- Again, we calculate RMSE, MAE, and R² to measure the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3a983-fd82-4dba-a371-b5acc340c3da",
   "metadata": {},
   "source": [
    "## **Step 5: Optimizing the Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf1c24-1b69-413d-8650-0292de0836d2",
   "metadata": {},
   "source": [
    "### **Random Forest Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157b1bf5-2d5b-4f25-aabd-59ddb921321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Optimized RMSE (Random Forest): 257.9967886088908\n",
      "Optimized MAE (Random Forest): 173.9562655541574\n",
      "Optimized R² (Random Forest): 0.8306632022025562\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Creating the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Here we define the hyperparameters we want to test\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters for Random Forest\n",
    "best_rf_params = grid_search_rf.best_params_\n",
    "print(f\"Best parameters for Random Forest: {best_rf_params}\")\n",
    "\n",
    "# Retraining Random Forest with the best parameters\n",
    "optimized_rf_model = RandomForestRegressor(n_estimators=best_rf_params['n_estimators'],\n",
    "                                           max_depth=best_rf_params['max_depth'],\n",
    "                                           min_samples_split=best_rf_params['min_samples_split'],\n",
    "                                           random_state=42)\n",
    "optimized_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the optimized model\n",
    "y_pred_rf_optimized = optimized_rf_model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE, MAE, and R² for the optimized model\n",
    "rmse_rf_optimized = mean_squared_error(y_test, y_pred_rf_optimized) ** 0.5\n",
    "mae_rf_optimized = mean_absolute_error(y_test, y_pred_rf_optimized)\n",
    "r2_rf_optimized = r2_score(y_test, y_pred_rf_optimized)\n",
    "\n",
    "print(f\"Optimized RMSE (Random Forest): {rmse_rf_optimized}\")\n",
    "print(f\"Optimized MAE (Random Forest): {mae_rf_optimized}\")\n",
    "print(f\"Optimized R² (Random Forest): {r2_rf_optimized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff211fdb-3d53-4f7a-bd4e-2f4ab05f1933",
   "metadata": {},
   "source": [
    "- **Hyperparameter Grid:** We define a range of hyperparameters (**n_estimators**, **max_depth**, **min_samples_split**) to optimize the model. This is to find the best combination of parameters that improves performance.\n",
    "- **Grid Search:** We use **GridSearchCV** to automatically search through the specified hyperparameters and evaluate the model's performance using cross-validation. It helps find the best parameters.\n",
    "- **Re-training:** After obtaining the best parameters, we re-train the Random Forest model with these settings to ensure that it performs at its best.\n",
    "- **Evaluation:** After training the optimized model, we calculate RMSE, MAE, and R² to evaluate how well the model has improved. RMSE and MAE help us measure the model's prediction accuracy, and R² tells us how much variance in the target variable is explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57f414-414d-48d1-9b81-8259c02c6029",
   "metadata": {},
   "source": [
    "### **XGBoost Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80a898e-eb37-4d18-bb79-b9f1bfbbff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Optimized RMSE (XGBoost): 253.71276666488424\n",
      "Optimized MAE (XGBoost): 176.1708221435547\n",
      "Optimized R² (XGBoost): 0.8362401723861694\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for tuning\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Creating the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Setting up the hyperparameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters for XGBoost\n",
    "best_xgb_params = grid_search_xgb.best_params_\n",
    "print(f\"Best parameters for XGBoost: {best_xgb_params}\")\n",
    "\n",
    "# Retraining XGBoost with the best parameters\n",
    "optimized_xgb_model = xgb.XGBRegressor(n_estimators=best_xgb_params['n_estimators'],\n",
    "                                       learning_rate=best_xgb_params['learning_rate'],\n",
    "                                       max_depth=best_xgb_params['max_depth'],\n",
    "                                       subsample=best_xgb_params['subsample'],\n",
    "                                       random_state=42)\n",
    "optimized_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the optimized model\n",
    "y_pred_xgb_optimized = optimized_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE, MAE, and R² for the optimized model\n",
    "rmse_xgb_optimized = mean_squared_error(y_test, y_pred_xgb_optimized) ** 0.5\n",
    "mae_xgb_optimized = mean_absolute_error(y_test, y_pred_xgb_optimized)\n",
    "r2_xgb_optimized = r2_score(y_test, y_pred_xgb_optimized)\n",
    "\n",
    "print(f\"Optimized RMSE (XGBoost): {rmse_xgb_optimized}\")\n",
    "print(f\"Optimized MAE (XGBoost): {mae_xgb_optimized}\")\n",
    "print(f\"Optimized R² (XGBoost): {r2_xgb_optimized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf8b25-3b4b-4a0f-af30-d899643cab85",
   "metadata": {},
   "source": [
    "- **Hyperparameter Grid:** In this section, we specify the hyperparameters (**n_estimators**, **learning_rate**, **max_depth**, **subsample**) that we will use to tune the XGBoost model.\n",
    "- **Grid Search:** Just like with Random Forest, we use **GridSearchCV** to optimize the hyperparameters. The cross-validation ensures the model is generalized and avoids overfitting.\n",
    "- **Re-training:** After identifying the best parameters, we train the model with them to ensure the model performs optimally on unseen data.\n",
    "- **Evaluation:** RMSE, MAE, and R² are used again here to measure the model’s performance. These metrics give us insights into the model’s accuracy and goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6e74a-1ccb-475b-b978-cc3d95a560f9",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "### **Why These Algorithms?**  \n",
    "Since our problem is a regression task, we chose Random Forest and XGBoost for their strong performance in numerical predictions.\n",
    "\n",
    "- Random Forest is a robust ensemble method that reduces overfitting and handles complex relationships in data by averaging multiple decision trees.\n",
    "- XGBoost is an optimized gradient boosting algorithm that improves accuracy by learning from previous mistakes, making it highly effective for structured data.\n",
    "\n",
    "Both models handle missing data well, reduce overfitting, and work efficiently with tabular datasets, making them ideal for predicting school rankings.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance Comparison**  \n",
    "To evaluate how well our models predict school rankings, we used the following metrics:  \n",
    "\n",
    "- **RMSE (Root Mean Squared Error)** – Measures how far the predictions are from actual rankings. A lower RMSE means fewer large mistakes. We chose RMSE because it penalizes big errors more, which is important when ranking schools.  \n",
    "\n",
    "- **MAE (Mean Absolute Error)** – Tells us the average size of the errors in ranking predictions. Unlike RMSE, it treats all errors equally, making it useful for understanding overall accuracy.  \n",
    "\n",
    "- **R² (R-Squared)** – Measures how well the model explains the variation in school rankings. A higher R² means the model is learning meaningful patterns instead of guessing.  \n",
    "\n",
    "These metrics help us quantify model performance, ensuring our predictions are both accurate and reliable.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Results**  \n",
    "\n",
    "| **Model**  | **RMSE (Lower is Better)** | **MAE (Lower is Better)** | **R² (Higher is Better)** |\n",
    "|------------|--------------------------|--------------------------|--------------------------|\n",
    "| **Random Forest** | 257.99 | 173.95 | 0.830 |\n",
    "| **XGBoost** | **253.71** | **176.17** | **0.836** |\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Decision: Best Model for Our Data**  \n",
    "\n",
    "Based on our evaluation, **XGBoost** performed the best because:  \n",
    "\n",
    " **Lowest RMSE** – It makes the smallest mistakes in ranking predictions.  \n",
    " **Highest R²** – It explains the most variance in the rankings.  \n",
    " **Better performance than Random Forest** – While both models are strong, XGBoost’s ability to focus on hard-to-predict rankings gives it an edge.  \n",
    "\n",
    "---\n",
    "\n",
    "## **References**\n",
    "\n",
    "\n",
    "- Breiman, L. (2001). *Random forests*. Machine Learning, 45(1), 5-32.  \n",
    "  [Springer: Random Forest Paper](https://link.springer.com/article/10.1023/A:1010933404324)  \n",
    "\n",
    "- **Scikit-learn Documentation:**  \n",
    "  [Random Forest Documentation](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)  \n",
    "\n",
    "\n",
    "- **XGBoost Official Documentation:**  \n",
    "  [XGBoost Docs](https://xgboost.readthedocs.io/en/stable/)  \n",
    "\n",
    "\n",
    "- Bergstra, J., & Bengio, Y. (2012). *Random Search for Hyper-Parameter Optimization*. JMLR, 13, 281-305.  \n",
    "  [JMLR Paper](https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)  \n",
    "\n",
    "- **Scikit-learn Grid Search Documentation:**  \n",
    "  [GridSearchCV](https://scikit-learn.org/stable/modules/grid_search.html)  \n",
    "\n",
    "\n",
    "- Willmott, C. J., & Matsuura, K. (2005). *Advantages of the Mean Absolute Error (MAE) over the Root Mean Square Error (RMSE)*. Climate Research, 30(1), 79-82.  \n",
    "   [Research Paper](https://www.int-res.com/articles/cr2005/30/c030p079.pdf)  \n",
    "\n",
    "- **Scikit-learn Metrics Documentation:**  \n",
    "  [Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e09cdc-f538-4af2-8c54-1be6c53a7711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
