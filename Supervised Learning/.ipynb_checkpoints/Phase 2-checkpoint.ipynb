{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9f501b-71c9-49ae-97e8-3921e2ab1589",
   "metadata": {},
   "source": [
    "## **Step 1: Loading the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35ecf77-982d-407a-ae4d-00cd391fce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>اسم_المدرسة</th>\n",
       "      <th>المنطقة_الإدارية</th>\n",
       "      <th>الإدارة_التعليمية</th>\n",
       "      <th>المكتب_التعليمي</th>\n",
       "      <th>السلطة</th>\n",
       "      <th>نوع_التعليم</th>\n",
       "      <th>الجنس</th>\n",
       "      <th>تخصص_الاختبار</th>\n",
       "      <th>متوسط_أداء_الطلبة_في_المدرسة</th>\n",
       "      <th>ترتيب_المدرسة_على_مستوى_المدارس</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>294</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2496</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.872710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3050</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>218</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991601</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1276</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850850</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3938</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   اسم_المدرسة  المنطقة_الإدارية  الإدارة_التعليمية  المكتب_التعليمي  السلطة  \\\n",
       "0           28                11                 45              294       3   \n",
       "1         2496                 3                 38               71       0   \n",
       "2         3050                11                 34              218       3   \n",
       "3         1276                 3                 38               71       3   \n",
       "4         3938                 4                 31              245       0   \n",
       "\n",
       "   نوع_التعليم  الجنس  تخصص_الاختبار  متوسط_أداء_الطلبة_في_المدرسة  \\\n",
       "0            6      0              0                      1.000000   \n",
       "1            6      0              1                      0.872710   \n",
       "2            6      0              0                      0.991601   \n",
       "3            0      0              1                      0.850850   \n",
       "4            6      0              0                      0.982336   \n",
       "\n",
       "   ترتيب_المدرسة_على_مستوى_المدارس  \n",
       "0                                1  \n",
       "1                                1  \n",
       "2                                2  \n",
       "3                                2  \n",
       "4                                3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we're importing pandas, it's the library we need to work with the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Now, we load the dataset from the CSV file using pandas\n",
    "df = pd.read_csv(r\"../Dataset/Cleaned_Averages.csv\")\n",
    "\n",
    "# We display the first few rows of the dataset to understand its structure and confirm that the data loaded correctly\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c283fe4-7c4a-4f8b-bd18-ddfdcd92a1ef",
   "metadata": {},
   "source": [
    "**Loading the Dataset:** In this step, we import the pandas library to handle the dataset. We use **pd.read_csv()** to load our data from a CSV file into a pandas DataFrame. Then, we call **df.head()** to display the first few rows of the dataset to confirm that it has loaded correctly and that the data structure looks fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431a3fb-7b40-4daf-82d1-b6bf58877814",
   "metadata": {},
   "source": [
    "## **Step 2: Splitting the Data into Training and Testing Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ce5a82-7e86-4be3-8112-4033a1095ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (5364, 7), Test set size: (1342, 7)\n"
     ]
    }
   ],
   "source": [
    "# We select the input features (X) \n",
    "X = df[['المنطقة_الإدارية', 'المكتب_التعليمي', 'السلطة', 'الإدارة_التعليمية', 'نوع_التعليم', 'الجنس', 'متوسط_أداء_الطلبة_في_المدرسة']]\n",
    "\n",
    "# The target (y) is what we want to predict\n",
    "y = df['ترتيب_المدرسة_على_مستوى_المدارس']  \n",
    "\n",
    "# Now, we split the data into training and testing sets (80% for training, 20% for testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Checking the size of the sets\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93738ac3-32f3-4419-935c-7f925875cf1f",
   "metadata": {},
   "source": [
    "**Splitting the Data:** After selecting the features and the target variable, we divide our data into training and test sets using **train_test_split()** from sklearn. The training set will consist of 80% of the data, and the remaining 20% will be used for testing the model. This is an important step because we want to train the model on one set of data and evaluate it on another to check how well it performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ea983-8f16-4fed-bbb5-302a6f8e9447",
   "metadata": {},
   "source": [
    "## **Step 3: Random Forest Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ea69c7-dc4b-416b-817e-fe04e5ba6314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Random Forest): 276.6023270118728\n",
      "MAE (Random Forest): 174.3432041728763\n",
      "R² (Random Forest): 0.8053589667131565\n"
     ]
    }
   ],
   "source": [
    "# First, we import RandomForestRegressor and some metrics we need for evaluation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Now, we create the Random Forest model\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training the model using the training data\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the test data\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Calculating RMSE (Root Mean Squared Error) to evaluate the model\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf)**0.5\n",
    "# Calculating MAE (Mean Absolute Error) to evaluate the model\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "# Calculating R² (R-squared) to evaluate the model\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"RMSE (Random Forest): {rmse_rf}\")\n",
    "print(f\"MAE (Random Forest): {mae_rf}\")\n",
    "print(f\"R² (Random Forest): {r2_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d9050-ad90-498f-b42c-7619dcc4fc58",
   "metadata": {},
   "source": [
    "**Random Forest Algorithm:** In this step, we use the Random Forest algorithm to build our model. We set **n_estimators=100** to use 100 trees in the forest and **random_state=42** for reproducibility. After training the model, we use it to predict the school rankings on the test set.\n",
    "\n",
    "**We calculate several evaluation metrics:**\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** This gives us an idea of how far off our predictions are on average. A lower RMSE indicates better performance.\n",
    "\n",
    "- **MAE (Mean Absolute Error):** This shows the average error in our predictions.\n",
    "\n",
    "- **R² (R-squared):** This tells us how well our model is explaining the variance in the target variable. A higher R² means the model fits the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45805f3a-e272-4a71-800d-1f1fce48d46b",
   "metadata": {},
   "source": [
    "## **Step 4: SVM Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52383ff8-1053-44ec-9196-43af503d56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (SVM): 621.4546953787732\n",
      "MAE (SVM): 530.3119733589066\n",
      "R² (SVM): 0.017479343136376113\n"
     ]
    }
   ],
   "source": [
    "# We import the SVM model and the necessary metrics for evaluation\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Now, let's create the SVM model\n",
    "model_svr = SVR()\n",
    "\n",
    "# Training the model with the training data\n",
    "model_svr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions using the test data\n",
    "y_pred_svr = model_svr.predict(X_test)\n",
    "\n",
    "# Calculating RMSE (Root Mean Squared Error) for SVM\n",
    "rmse_svr = mean_squared_error(y_test, y_pred_svr)**0.5\n",
    "# Calculating MAE (Mean Absolute Error) for SVM\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "# Calculating R² (R-squared) for SVM\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"RMSE (SVM): {rmse_svr}\")\n",
    "print(f\"MAE (SVM): {mae_svr}\")\n",
    "print(f\"R² (SVM): {r2_svr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8da55-16f6-4e38-95f1-e4dd0d43c3fc",
   "metadata": {},
   "source": [
    "- **SVM Algorithm:** In this step, we implement the Support Vector Machine (SVM) model using SVR from sklearn. This is a regression model that tries to find a hyperplane in a higher-dimensional space to make predictions. After training, we predict the rankings of schools in the test data.\n",
    "- We calculate the same evaluation metrics as before: RMSE, MAE, and R²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc04b37-90ba-41d2-895e-2d9716cdf190",
   "metadata": {},
   "source": [
    "## **Step 5: XGBoost Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41c5056-5981-4a67-88c9-bf7330050019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (XGBoost): 288.62883009585164\n",
      "MAE (XGBoost): 180.7254638671875\n",
      "R² (XGBoost): 0.7880652546882629\n"
     ]
    }
   ],
   "source": [
    "# We import the XGBoost model and the metrics for evaluation\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Creating the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Training the model using the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions using the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE (Root Mean Squared Error) for XGBoost\n",
    "rmse_xgb = mean_squared_error(y_test, y_pred_xgb)**0.5\n",
    "# Calculating MAE (Mean Absolute Error) for XGBoost\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "# Calculating R² (R-squared) for XGBoost\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Displaying the results\n",
    "print(f\"RMSE (XGBoost): {rmse_xgb}\")\n",
    "print(f\"MAE (XGBoost): {mae_xgb}\")\n",
    "print(f\"R² (XGBoost): {r2_xgb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e39b7-f6b2-47dd-ace1-2a4a0bec961c",
   "metadata": {},
   "source": [
    "- **XGBoost Algorithm:** In this step, we are using the XGBoost model, which is known for being highly effective in many regression tasks. We train the model using the training data and make predictions using the test data.\n",
    "- Again, we calculate RMSE, MAE, and R² to measure the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3a983-fd82-4dba-a371-b5acc340c3da",
   "metadata": {},
   "source": [
    "## **Step 6: Optimizing the Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf1c24-1b69-413d-8650-0292de0836d2",
   "metadata": {},
   "source": [
    "### **Random Forest Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157b1bf5-2d5b-4f25-aabd-59ddb921321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Optimized RMSE (Random Forest): 257.9967886088908\n",
      "Optimized MAE (Random Forest): 173.9562655541574\n",
      "Optimized R² (Random Forest): 0.8306632022025562\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Creating the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Here we define the hyperparameters we want to test\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters for Random Forest\n",
    "best_rf_params = grid_search_rf.best_params_\n",
    "print(f\"Best parameters for Random Forest: {best_rf_params}\")\n",
    "\n",
    "# Retraining Random Forest with the best parameters\n",
    "optimized_rf_model = RandomForestRegressor(n_estimators=best_rf_params['n_estimators'],\n",
    "                                           max_depth=best_rf_params['max_depth'],\n",
    "                                           min_samples_split=best_rf_params['min_samples_split'],\n",
    "                                           random_state=42)\n",
    "optimized_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the optimized model\n",
    "y_pred_rf_optimized = optimized_rf_model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE, MAE, and R² for the optimized model\n",
    "rmse_rf_optimized = mean_squared_error(y_test, y_pred_rf_optimized) ** 0.5\n",
    "mae_rf_optimized = mean_absolute_error(y_test, y_pred_rf_optimized)\n",
    "r2_rf_optimized = r2_score(y_test, y_pred_rf_optimized)\n",
    "\n",
    "print(f\"Optimized RMSE (Random Forest): {rmse_rf_optimized}\")\n",
    "print(f\"Optimized MAE (Random Forest): {mae_rf_optimized}\")\n",
    "print(f\"Optimized R² (Random Forest): {r2_rf_optimized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff211fdb-3d53-4f7a-bd4e-2f4ab05f1933",
   "metadata": {},
   "source": [
    "- **Hyperparameter Grid:** We define a range of hyperparameters (**n_estimators**, **max_depth**, **min_samples_split**) to optimize the model. This is to find the best combination of parameters that improves performance.\n",
    "- **Grid Search:** We use **GridSearchCV** to automatically search through the specified hyperparameters and evaluate the model's performance using cross-validation. It helps find the best parameters.\n",
    "- **Re-training:** After obtaining the best parameters, we re-train the Random Forest model with these settings to ensure that it performs at its best.\n",
    "- **Evaluation:** After training the optimized model, we calculate RMSE, MAE, and R² to evaluate how well the model has improved. RMSE and MAE help us measure the model's prediction accuracy, and R² tells us how much variance in the target variable is explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57f414-414d-48d1-9b81-8259c02c6029",
   "metadata": {},
   "source": [
    "### **XGBoost Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80a898e-eb37-4d18-bb79-b9f1bfbbff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Optimized RMSE (XGBoost): 253.71276666488424\n",
      "Optimized MAE (XGBoost): 176.1708221435547\n",
      "Optimized R² (XGBoost): 0.8362401723861694\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for tuning\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Creating the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Setting up the hyperparameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters for XGBoost\n",
    "best_xgb_params = grid_search_xgb.best_params_\n",
    "print(f\"Best parameters for XGBoost: {best_xgb_params}\")\n",
    "\n",
    "# Retraining XGBoost with the best parameters\n",
    "optimized_xgb_model = xgb.XGBRegressor(n_estimators=best_xgb_params['n_estimators'],\n",
    "                                       learning_rate=best_xgb_params['learning_rate'],\n",
    "                                       max_depth=best_xgb_params['max_depth'],\n",
    "                                       subsample=best_xgb_params['subsample'],\n",
    "                                       random_state=42)\n",
    "optimized_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the optimized model\n",
    "y_pred_xgb_optimized = optimized_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE, MAE, and R² for the optimized model\n",
    "rmse_xgb_optimized = mean_squared_error(y_test, y_pred_xgb_optimized) ** 0.5\n",
    "mae_xgb_optimized = mean_absolute_error(y_test, y_pred_xgb_optimized)\n",
    "r2_xgb_optimized = r2_score(y_test, y_pred_xgb_optimized)\n",
    "\n",
    "print(f\"Optimized RMSE (XGBoost): {rmse_xgb_optimized}\")\n",
    "print(f\"Optimized MAE (XGBoost): {mae_xgb_optimized}\")\n",
    "print(f\"Optimized R² (XGBoost): {r2_xgb_optimized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf8b25-3b4b-4a0f-af30-d899643cab85",
   "metadata": {},
   "source": [
    "- **Hyperparameter Grid:** In this section, we specify the hyperparameters (**n_estimators**, **learning_rate**, **max_depth**, **subsample**) that we will use to tune the XGBoost model.\n",
    "- **Grid Search:** Just like with Random Forest, we use **GridSearchCV** to optimize the hyperparameters. The cross-validation ensures the model is generalized and avoids overfitting.\n",
    "- **Re-training:** After identifying the best parameters, we train the model with them to ensure the model performs optimally on unseen data.\n",
    "- **Evaluation:** RMSE, MAE, and R² are used again here to measure the model’s performance. These metrics give us insights into the model’s accuracy and goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f818b-6308-4603-8b84-41fe4809e67c",
   "metadata": {},
   "source": [
    "### **SVM Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04af663-9738-4e48-a947-7183d32ee584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for tuning\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Creating the SVM model\n",
    "svm_model = SVR()\n",
    "\n",
    "# Setting up the hyperparameter grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [1, 10, 100],\n",
    "    'epsilon': [0.1, 0.2, 0.3],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid_svm, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters for SVM\n",
    "best_svm_params = grid_search_svm.best_params_\n",
    "print(f\"Best parameters for SVM: {best_svm_params}\")\n",
    "\n",
    "# Retraining SVM with the best parameters\n",
    "optimized_svm_model = SVR(C=best_svm_params['C'], epsilon=best_svm_params['epsilon'], kernel=best_svm_params['kernel'])\n",
    "optimized_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the optimized model\n",
    "y_pred_svm_optimized = optimized_svm_model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE, MAE, and R² for the optimized model\n",
    "rmse_svm_optimized = mean_squared_error(y_test, y_pred_svm_optimized) ** 0.5\n",
    "mae_svm_optimized = mean_absolute_error(y_test, y_pred_svm_optimized)\n",
    "r2_svm_optimized = r2_score(y_test, y_pred_svm_optimized)\n",
    "\n",
    "print(f\"Optimized RMSE (SVM): {rmse_svm_optimized}\")\n",
    "print(f\"Optimized MAE (SVM): {mae_svm_optimized}\")\n",
    "print(f\"Optimized R² (SVM): {r2_svm_optimized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb06a39-66e0-4c36-895d-6732fc139fd3",
   "metadata": {},
   "source": [
    "- **Hyperparameter Grid:** This is the section where we define which hyperparameters of the SVM model to tune. The most important ones here are **C**, **epsilon**, and **kernel**, as they directly affect how well the model generalizes.\n",
    "- **Grid Search:** As in the other models, we use **GridSearchCV** to explore multiple hyperparameter combinations and use cross-validation to ensure that the model performs optimally.\n",
    "- **Re-training:** After the best parameters are found, we re-train the SVM model with those parameters and make predictions on the test data.\n",
    "- **Evaluation:** The evaluation metrics (RMSE, MAE, and R²) give us a sense of how well the SVM model is performing on the test data. These metrics tell us how far off the predictions are and how well the model fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6e74a-1ccb-475b-b978-cc3d95a560f9",
   "metadata": {},
   "source": [
    "## **Conclusion:**\n",
    "### **Why These Algorithms?**\n",
    "\n",
    "We chose Random Forest, XGBoost, and SVM because they are all powerful machine learning algorithms that have proven to be effective in regression tasks. Both Random Forest and XGBoost are ensemble methods, which means they combine multiple models (trees) to improve performance and increase predictive accuracy. These methods are especially useful when dealing with complex datasets. On the other hand, SVM is a solid option for regression, particularly when the dataset is smaller and has non-linear relationships between the features.\n",
    "\n",
    "\n",
    "### **Performance Comparison:**\n",
    "To evaluate the effectiveness of these algorithms, we compared them based on the following metrics: RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), and R² (Coefficient of Determination). Lower RMSE and MAE values indicate better predictive accuracy, while a higher R² suggests a better fit to the data. These metrics help us understand how well each model performs when predicting the school rankings.\n",
    "\n",
    "### **Results:**\n",
    "\n",
    "**Random Forest:**\n",
    "\n",
    "- RMSE: 257.99\n",
    "- MAE: 173.95\n",
    "- R²: 0.830\n",
    "  \n",
    "**XGBoost:**\n",
    "\n",
    "- RMSE: 253.71\n",
    "- MAE: 176.17\n",
    "- R²: 0.836\n",
    "\n",
    "**SVM:**\n",
    "\n",
    "- RMSE: \n",
    "- MAE: \n",
    "- R²: \n",
    "\n",
    "### **Best Model:**\n",
    "Based on the performance comparison, XGBoost emerges as the best model for predicting school rankings. Here's why:\n",
    "\n",
    "- **Lowest RMSE:** XGBoost has the lowest RMSE, which indicates the smallest average error between the predicted and actual values.\n",
    "Highest R²: XGBoost also has the highest R², meaning it explains the most variance in the data, showing that it fits the data better than the other models.\n",
    "- **MAE:** While XGBoost's MAE is slightly higher than Random Forest, it is still much better than SVM in terms of predictive accuracy.\n",
    "  \n",
    "### **Conclusion:**\n",
    "In summary, XGBoost stands out as the most optimized model with the best balance between RMSE, MAE, and R². Therefore, XGBoost is the recommended model to use for this task, as it provides the most accurate predictions and best data fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb0891-70ab-4c03-8783-239e7d20e89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
